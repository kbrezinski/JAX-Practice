{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kbrezinski/JAX-Practice/blob/main/nn_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xFgbl10jQDu"
   },
   "source": [
    "# MLP Training on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "2RQonGr7jThC"
   },
   "outputs": [],
   "source": [
    "# init MLP and predict()\n",
    "# torch dataloader\n",
    "# training loop, loss fn\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax import jit, vmap, pmap, grad\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFt9h0bBjsWQ",
    "outputId": "0bd8d585-ef80-4418-bde5-5d3cb27fe503"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(512, 784), (512,)], [(256, 512), (256,)], [(10, 256), (10,)]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constants\n",
    "seed = 0\n",
    "mnist_img_size = 784\n",
    "\n",
    "def init_MLP(layer_widths, parent_key, scale=0.01):\n",
    "\n",
    "  params = []\n",
    "  keys = jax.random.split(parent_key, num=len(layer_widths) - 1)\n",
    "\n",
    "  for in_width, out_width, key in zip(layer_widths[:-1], layer_widths[1:], keys):\n",
    "\n",
    "    weight_key, bias_key = jax.random.split(key)\n",
    "    params.append(\n",
    "        [scale * jax.random.normal(weight_key, shape=(out_width, in_width)), # weights = (hidden dims, input)\n",
    "         scale * jax.random.normal(bias_key, shape=(out_width,))]           # biases = (hidden_dims, 1)\n",
    "    )\n",
    "  return params\n",
    "\n",
    "parent_key = jax.jax.random.PRNGKey(seed)\n",
    "MLP_params = init_MLP([784, 512, 256, 10], parent_key)\n",
    "jax.tree_map(lambda x: x.shape, MLP_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "UuvWinf1kJMz"
   },
   "outputs": [],
   "source": [
    "def predict_MLP(params, x):\n",
    "\n",
    "  hidden_layers = params[:-1]\n",
    "\n",
    "  activation = x\n",
    "  for w, b in hidden_layers:\n",
    "    # ReLu( dot((512, 784)(784, 1)))\n",
    "    activation = jax.nn.relu(jnp.dot(w, activation) + b)\n",
    "\n",
    "  w_last, b_last = params[-1]\n",
    "  logits = jnp.dot(w_last, activation) + b_last\n",
    "\n",
    "  # mimics the same behavior as the softmax\n",
    "  return logits - logsumexp(logits) # log(exp(o1)) - log(sum(exp(01), exp(02), ..., exp(03)))\n",
    "\n",
    "dummy_img_flat = np.random.randn(mnist_img_size)\n",
    "assert dummy_img_flat.shape == (784,)\n",
    "\n",
    "prediction = predict_MLP(MLP_params, dummy_img_flat)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPODYGOZoqIX"
   },
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOPfzR1GcRhKImlvS75I/6+",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "nn_scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

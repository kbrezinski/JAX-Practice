{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOpO5BNim1oU/xG2ILAzb84",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kbrezinski/JAX-Practice/blob/main/nn_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xFgbl10jQDu"
      },
      "source": [
        "# MLP Training on MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RQonGr7jThC"
      },
      "source": [
        "# init MLP and predict()\n",
        "# torch dataloader\n",
        "# training loop, loss fn\n",
        "import jax\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from jax.scipy.special import logsumexp\n",
        "from jax import jit, vmap, pmap, grad"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFt9h0bBjsWQ",
        "outputId": "376cd71a-f904-4210-ddfa-ea261158405d"
      },
      "source": [
        "# constants\n",
        "seed = 0\n",
        "mnist_img_size = 784\n",
        "\n",
        "def init_MLP(layer_widths, parent_key, scale=0.01):\n",
        "\n",
        "  params = []\n",
        "  keys = jax.random.split(parent_key, num=len(layer_widths) - 1)\n",
        "\n",
        "  for in_width, out_width, key in zip(layer_widths[:-1], layer_widths[1:], keys):\n",
        "\n",
        "    weight_key, bias_key = jax.random.split(key)\n",
        "    params.append(\n",
        "        [scale * jax.random.normal(weight_key, shape=(out_width, in_width)), # weights = (hidden dims, input)\n",
        "         scale * jax.random.normal(bias_key, shape=(out_width,))]           # biases = (hidden_dims, 1)\n",
        "    )\n",
        "  return params\n",
        "\n",
        "parent_key = jax.jax.random.PRNGKey(seed)\n",
        "MLP_params = init_MLP([784, 512, 256, 10], parent_key)\n",
        "jax.tree_map(lambda x: x.shape, MLP_params)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(512, 784), (512,)], [(256, 512), (256,)], [(10, 256), (10,)]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuvWinf1kJMz"
      },
      "source": [
        "def predict_MLP(params, x):\n",
        "\n",
        "  hidden_layers = params[:-1]\n",
        "\n",
        "  activation = x\n",
        "  for w, b in hidden_layers:\n",
        "    # ReLu( dot((512, 784)(784, 1)))\n",
        "    activation = jax.nn.relu(jnp.dot(w, activation) + b)\n",
        "\n",
        "  w_last, b_last = params[-1]\n",
        "  logits = jnp.dot(w_last, activation) + b_last\n",
        "\n",
        "  # mimics the same behavior as the softmax\n",
        "  return logits - logsumexp(logits) # log(exp(o1)) - log(sum(exp(01), exp(02), ..., exp(03)))\n",
        "\n",
        "# test on single image dimension (1, 784)\n",
        "dummy_img_flat = np.random.randn(np.prod(mnist_img_size))\n",
        "assert dummy_img_flat.shape == (784,)\n",
        "prediction = predict_MLP(MLP_params, dummy_img_flat)\n",
        "assert prediction.shape == (10,)\n",
        "\n",
        "# test on a batched dimension (17, 784)\n",
        "dummy_imgs_flats = np.random.randn(16, np.prod(mnist_img_size))\n",
        "batched_MLP_predict = vmap(predict_MLP, in_axes=(None, 0))  # None to broadcast only params, batched dimension on zeroth dimension set to 0\n",
        "predictions = batched_MLP_predict(MLP_params, dummy_imgs_flats)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPODYGOZoqIX"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "## adding the dataloader\n",
        "\n",
        "batch_size = 32\n",
        "transform_ds = lambda x: np.ravel(np.array(x, dtype=np.float32) / 255.)\n",
        "\n",
        "def collate_custom(batch):\n",
        "  transposed_data = list(zip(*batch)) \n",
        "  return np.array(transposed_data[0]), np.array(transposed_data[1])\n",
        "\n",
        "train_ds = MNIST(root='train_mnist', train=True, download=True, transform=transform_ds)\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, collate_fn=collate_custom)\n",
        "\n",
        "test_ds = MNIST(root='test_mnist', train=False, download=True, transform=transform_ds)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_dl))"
      ],
      "metadata": {
        "id": "LVZ3b2M5G_JV",
        "outputId": "ce8857c9-71a1-4c57-d394-d26af399857a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
              " array([1, 5, 8, 1, 6, 5, 7, 0, 9, 6, 7, 8, 2, 6, 8, 8, 8, 7, 6, 0, 2, 4,\n",
              "        1, 8, 8, 9, 9, 9, 1, 3, 6, 5]))"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_1un1e4RO1y_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
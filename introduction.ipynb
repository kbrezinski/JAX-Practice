{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPm8FMN2H6Wce0nFqDkOEMm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kbrezinski/JAX-Practice/blob/main/introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT523StdXdax"
      },
      "source": [
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "# special transformations\n",
        "from jax import grad, jit, vmap, pmap\n",
        "# JAX low level APIs\n",
        "from jax import lax, make_jaxpr, random, device_put"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXbdBPdqYKnC"
      },
      "source": [
        "# show comptability with numpy arrays\n",
        "x = jnp.linspace(0, 10, 1000)\n",
        "y = 2 * np.sin(x) * np.cos(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYzcR3pTYfYs",
        "outputId": "edb5a367-ad67-4c09-e6d4-abc983f385c0"
      },
      "source": [
        "# how to set values on immutable JAX arrays\n",
        "x = jnp.arange(10)\n",
        "y = x.at[4].set(-1) \n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([ 0,  1,  2,  3, -1,  5,  6,  7,  8,  9], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kF5ltkPY_Ha",
        "outputId": "710f7058-4a34-4684-c201-fdf17ef1d9de"
      },
      "source": [
        "# in Jax random numbers are stateful, need to be passed in as args\n",
        "k = random.PRNGKey(2021)\n",
        "x = random.normal(k, (10,))\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([-1.906434  ,  0.94750565, -0.04492167, -0.5956922 ,\n",
              "              0.9293699 , -0.3326311 , -2.6711135 ,  0.6385124 ,\n",
              "              0.5522837 ,  1.7470939 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJURTQ9RZZ2-",
        "outputId": "69a063d3-a4ab-4425-9cbb-ae2d1aaaaa44"
      },
      "source": [
        "x = random.normal(k, (4, 4), dtype=jnp.float32) # automatically cast to acccelerated by default\n",
        "%timeit jnp.dot(x, x.T).block_until_ready() # runs faster than numpy because its being accelerated on GPU and no overhead\n",
        "x = device_put(np.random.normal(size=(4, 4))).astype(np.float32) # push numpy to GPU\n",
        "\n",
        "# block_until_ready() waits until the completion is done using asynchronous dispatch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 8.31 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000 loops, best of 5: 322 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVky8tCMbPhS",
        "outputId": "226744af-9681-4fc2-c89e-5bc974a0e9b9"
      },
      "source": [
        "# jit caches the intermediate results and make computation faster\n",
        "def selu(x, a=1.67, lmbda=1.05):\n",
        "  return lmbda * jnp.where(x > 0, x, a * jnp.exp(x) - a)\n",
        "\n",
        "selu_jit = jit(selu)\n",
        "x = random.normal(k, (1_000_000,))\n",
        "%timeit selu_jit(x).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 673.03 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "10000 loops, best of 5: 113 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvZKtumRdY1U",
        "outputId": "48132fe7-c666-41f0-e55e-56da4b22f55c"
      },
      "source": [
        "def sum_logistic(x):\n",
        "  #return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
        "  return jnp.sum(x**2) # 2*x1 + 2*x2 + 2*x3\n",
        "\n",
        "# [1, 2, 3]\n",
        "x = jnp.arange(3.)\n",
        "loss = sum_logistic # rename\n",
        "\n",
        "# wrap loss fn around grad\n",
        "grad_loss = grad(loss)\n",
        "# determine loss\n",
        "grad_loss(x) # why does it output as an array?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([0., 2., 4.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0PgttRSeYnM",
        "outputId": "35ecd0ac-5aee-4499-bdb1-88a1e01a9432"
      },
      "source": [
        "x = 1.\n",
        "y = 1.\n",
        "\n",
        "# function is a product of x and y\n",
        "f = lambda x, y: x**2 + x + 4 + y**2\n",
        "\n",
        "# passing in argnums will tell grad to differentiate wrt y instead\n",
        "dfdx = grad(f, argnums=(1)) # 2*x + 1\n",
        "d2fdx = grad(dfdx, argnums=(1)) # 2\n",
        "d3fdx = grad(d2fdx, argnums=(1)) # 0\n",
        "\n",
        "print(f(x, y), dfdx(x, y), d2fdx(x, y), d3fdx(x, y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.0 2.0 2.0 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNr_Or_biH0l",
        "outputId": "d677f3f7-3de6-4777-bbcf-f4330e96f1fb"
      },
      "source": [
        "from jax import jacfwd, jacrev # find jacobians for deritives\n",
        "\n",
        "f = lambda x, y: x**2 + y**2\n",
        "# Jacobian = [df/dx, df/dy]\n",
        "# Hessian = [[d2f/dx, d2f/dxdy], [d2f/dydx, d2f/dy]]\n",
        "\n",
        "def hessian(f):\n",
        "  return jit(jacfwd(jacrev(f, argnums=(0,1)), argnums=(0,1)))\n",
        "\n",
        "jacrev(f, argnums=(0,1))(1.,1.) # Jacobian = [2, 2]\n",
        "hessian(f)(1.,1.) # Hessian [[2,0],[0,2]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((DeviceArray(2., dtype=float32), DeviceArray(0., dtype=float32)),\n",
              " (DeviceArray(0., dtype=float32), DeviceArray(2., dtype=float32)))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC41OKDFkJyA",
        "outputId": "e0e8e0f1-b175-48e7-bcb8-35d156ce791f"
      },
      "source": [
        "f = lambda x: abs(x)\n",
        "\n",
        "# grad is smart enough to take the derivitive wrt x of the abs(0.) and return 1 even though its not differentiable\n",
        "dfdx = grad(f)\n",
        "print(dfdx(0.), dfdx(-1.))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0 -1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9cGyZAUkTaT",
        "outputId": "6f45f8cc-f5b9-47dc-e8c6-1fda739dd249"
      },
      "source": [
        "W = random.normal(k, (150, 100)) # weights that are of size (150, 100)\n",
        "batched_x = random.normal(k, (10, 100)) # 10 samples of 100 features\n",
        "\n",
        "# slow implementation, will not work because of sizes\n",
        "def apply_matrix(W, x):\n",
        "  return jnp.dot(W, x)\n",
        "\n",
        "# very slow using loops, need to vectorize\n",
        "def naively_batched_apply_matrix(batched_x):\n",
        "  return jnp.stack([apply_matrix(x) for x in batched_x])\n",
        "\n",
        "# really fast jit; does not account for single or batched sizes of 1 or 100\n",
        "@jit\n",
        "def batched_apply_matrix(batched_x):\n",
        "  return jnp.dot(batched_x, W.T)\n",
        "\n",
        "# apply vmap to single instance fn, then use batched on it.\n",
        "@jit\n",
        "def vmap_batched_apply_matrix(W, batched_x):\n",
        "  return vmap(apply_matrix, in_axes=(None, 0))(W, batched_x)\n",
        "\n",
        "%timeit vmap_batched_apply_matrix(W, batched_x).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 640.23 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "10000 loops, best of 5: 33.9 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iOISRnNlIzf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}